
\chapter{Markov Decision Process}

% https://trunghng.github.io/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html


\subsection{What is Reinforcement Learning?}

Say, there is an unknown environment that we're trying to put an agent on. By 
interacting with the agent through taking actions that gives rise to rewards 
continually, the agent learns a policy that maximize the cumulative rewards.

Reinforcement Learning (RL), roughly speaking, is an area of Machine Learning 
that describes methods aimed to learn a good strategy (called policy) for the 
agent from experimental trials and relative simple feedback received. With the 
optimal policy, the agent is capable to actively adapt to the environment to
maximize future rewards.

\subsection{Return}

The goal of agent is to maximize the cumulative reward in the long run. In 
general, we seek to maximize the expected return.

\begin{definition} {\rm\bf (Return)}
The return $G_t$ is the total discounted reward from $t$
\begin{equation}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1},
\end{equation}
where $\gamma\in[0,1]$ is called discount rate (or discount factor).
\end{definition}

The discount rate $\gamma$ determines the present value of future rewards: a 
reward received $k$ time steps in the future is worth only $\gamma^{k-1}$ times 
what it would be worth if it were received immediately. And also, it provides 
mathematical convenience since as $k\rightarrow\infty$ then $\gamma^k\rightarrow 0$.


\subsection{Policy}

Policy, which is denoted as $\pi$, is the behaviour function of the agent. $\pi$ 
is a mapping from states to probabilities of selecting each possible action. In 
other words, it lets us know which action to take in the current state $s$ and can 
be either deterministic or stochastic.

\begin{itemize}
%\setlength{\itemsep}{0pt}
%\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item[-]
Deterministic policy:
$\quad\pi(s)=a$

\item[-]
Stochastic policy: 
$\quad\pi(a|s)=P(A_t=a|S_t=s)$

\end{itemize}


\subsection{Value Function}

Value function measures how good a particular state is (or how good it is to 
perform a given action in a given state).

\begin{definition} {\rm\bf (state-value function)}
The state-value function of a state $s$ under a policy $\pi$, denoted as $v_\pi(s)$, 
is the expected return starting from state $s$ and following $\pi$ thereafter:
\begin{equation}
v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]
\end{equation}
\end{definition}

状态值函数 $v_\pi(s)$ 可以评价当前状态的好坏，每个状态的值不仅由当前状态决定还要由后面的状态决定，
所以状态的累计奖励求期望就可得出当前$s$的状态值函数 $v_\pi(s)$。

\begin{definition} {\rm\bf (action-value function)}
Similarly, we define the value of taking action a in state $s$ under a policy $\pi$, 
denoted as $q_\pi(s,a)$, as the expected return starting from $s$, taking the 
action $a$, and thereafter following policy $\pi$:
\begin{equation}
q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]
\end{equation}
Since we follow the policy $\pi$, we have that
\begin{equation}
v_\pi(s)=\sum_{a\in\mathcal{A}}q_\pi(s,a)\pi(a|s)
\end{equation}
\end{definition}



\section{Markov Decision Processes}

{\bf Markov decision processes (MDPs)} formally describe an environment for RL. 
And almost all RL problems can be formalised as MDPs.

\begin{definition} {\rm\bf (MDP)}
A Markov Decision Process is a tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, 
\mathcal{R}, \gamma>$
\begin{itemize}
%\setlength{\itemsep}{0pt}
%\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item[-]
$\mathcal{S}$ is a set of states called state space

\item[-]
$\mathcal{A}$ is a set of actions called action space

\item[-]
$\mathcal{P}$ is a state transition probability matrix \\
$\mathcal{P}^a_{ss'}=P(S_{t+1}=s'|S_t=s,A_t=a)$

\item[-]
$\mathcal{R}$ is a reward function \\
$ \mathcal{R}^a_s=\mathbb{E}\left[R_{t+1}|S_t=s,A_t=a\right]$

\item[-]
$\gamma\in[0, 1]$ is a discount factor for future reward

\end{itemize}
\end{definition}


\begin{definition}\label{def_FMDP} {\rm\bf (FMDP)}
A Finite Markov Decision Process is given a tuple $<\mathcal{S}, \mathcal{A}, \mathcal{T}, 
\mathcal{R}, H, \gamma>$
\begin{itemize}
%\setlength{\itemsep}{0pt}
%\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item[-]
Set of states $\mathcal{S}$

\item[-]
Set of actions $\mathcal{A}$

\item[-]
$\mathcal{T}: \mathcal{S}\times\mathcal{A}\times\mathcal{S}\times\{0,1,\ldots,H\}
\rightarrow [0,1]$ \\
$\mathcal{T}_t(s,a,s')=P(s_{t+1}=s'|s_t=s,a_t=a)$

\item[-]
$R$: \mathcal{S}\times\mathcal{A}\times\mathcal{S}\times\{0,1,\ldots,H\}
\rightarrow \mathcal{R} \\
$R_t(s,a,s')=\text{reward for} (s_{t+1}=s', s_t=s, a_t=a)$

\item[-]
$H$: horizon over which the agent will act

\item[-]
$\gamma\in[0, 1]$ is a discount factor for future reward

\end{itemize}
\end{definition}

Goal: Find $\pi: \mathcal{S}\times\{0,1,\ldots,H\}\rightarrow \mathcal{A}$ that 
maximizes expected sum of rewards, i.e., 
$$
\pi^*=\arg\max_\pi E\left[ \sum_{t=0}^H R_t(s_t, a_t, s_{t+1}) | \pi \right]
$$


\subsection{Optimal Policy and Optimal Value Function}

For finite MDPs (finite state and action space), we can precisely define an 
optimal policy. Value functions define a partial ordering over policies. A 
policy $\pi$ is defined to be better than or equal to a policy $\pi'$ if its 
expected return is greater than or equal to that of $\pi'$ for all states. In 
other words, 
\begin{equation} 
\pi\geq\pi'\iff v_\pi(s)\geq v_{\pi'} \forall s\in\mathcal{S} 
\end{equation}

\begin{theorem} {\rm\bf (Optimal policy)}
For any MDP, there exists an optimal policy $\pi_*$ that is better than or equal 
to all other policies, 
\begin{equation} 
\pi_*\geq\pi,\forall\pi 
\end{equation}
\end{theorem}

The proof of the above theorem is gonna be provided in another section since we 
need some additional tools to do that.

There may be more than one optimal policy, they share the same state-value function, 
called optimal state-value function though. 
\begin{equation} 
v_*(s)=\max_{\pi}v_\pi(s) 
\end{equation} 
Optimal policies also share the same action-value function, call optimal action-value 
function 
\begin{equation} 
q_*(s,a)=\max_{\pi}q_\pi(s,a) 
\end{equation}


\subsection{Bellman Equations}

A fundamental property of value functions used throughout RL is that they satisfy 
recursive relationships 
\begin{align*} 
v_\pi(s)&\doteq \mathbb{E}_\pi[G_t|S_t=s] \\
&=\mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s] \\
&=\sum_{s',r,g',a}p(s',r,g',a|s)(r+\gamma g') \\
&=\sum_{a}p(a|s)\sum_{s',r,g'}p(s',r,g'|a,s)(r+\gamma g') \\
&=\sum_{a}\pi(a|s)\sum_{s',r,g'}p(s',r|a,s)p(g'|s',r,a,s)(r+\gamma g') \\
&=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|a,s)\sum_{g'}p(g'|s')(r+\gamma g') \\
&=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|a,s)\left[r+\gamma\sum_{g'}p(g'|s')g'\right] \\
&=\sum_{a}\pi(a|s)\sum_{s',r}p(s',r|a,s)\left[r+\gamma v_\pi(s')\right], 
\end{align} 
where $p(s',r|s,a)=P(S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a)$, which defines the dynamics 
of the MDP. The last equation is called the Bellman equation for $v_\pi(s)$. It 
expresses a relationship between the value state $s$, $v_\pi(s)$ and the values of 
its successor states $s'$, $v_\pi(s')$.

Similarly, we define the Bellman equation for $q_\pi(s,a)$ 
\begin{align*} 
q_\pi(s,a)&\doteq\mathbb{E}_\pi[G_t|S_t=s,A_t=a] \\
&=\mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s,A_t=a] \\
&=\sum_{s',r}p(s',r|s,a)\left[r+\gamma\sum_{a'}\pi(a'|s')q_\pi(s',a')\right] 
\end{align}


\subsection{Bellman Backup Diagram}

Backup diagram of state-value function and action-value function respectively


\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1,
  level distance = 1.5cm}] 
\node [arn_r] {}
    child{ node [arn_n] (test0) {}
        {
            child{ node [arn_r] {}
                {
                edge from parent node[above left] {$r$}
                edge from parent node[below left] (A) { \makebox[6em][l]{$v_\pi(s') \leftarrowtail s'$} }
                }
            }
            child{ node [arn_r] {} }
            edge from parent node[below left] (A) { \makebox[4em][l]{$a$} }
        }% edge from parent node[left] {$r$} %for a named pointer
    }
    child{ node [arn_n] (test1) {}
            child{ node [arn_r] {} }
            child{ node [arn_r] {} }
	}edge from parent node[left] (A) {\makebox[5em][l]{$v_\pi(s) \leftarrowtail s$}}
    
    ;

    %\filldraw[red] (A) circle[radius=1pt];
\end{tikzpicture}



\begin{tikzpicture}[->,>=stealth',level/.style={sibling distance = 5cm/#1,
  level distance = 1.5cm}] 
\node [arn_n] {}
    child{ node [arn_r] (test0) {}
        {
            child{ node [arn_n] {}
                {
                % edge from parent node[above left] {$r$}
                edge from parent node[below left] (A) { \makebox[8em][l]{$q_\pi(s',a') \leftarrowtail a'$} }
                }
            }
            child{ node [arn_n] {} }
            edge from parent node[below left] (A) { \makebox[4em][l]{$s'$} }
        } edge from parent node[left] {$r$} %for a named pointer
    }
    child{ node [arn_r] (test1) {}
            child{ node [arn_n] {} }
            child{ node [arn_n] {} }
	}edge from parent node[left] (A) {\makebox[7em][l]{$q_\pi(s,a) \leftarrowtail s, a$}}
    
    ;

    %\filldraw[red] (A) circle[radius=1pt];
\end{tikzpicture}


\subsection{Bellman Optimality Equations}

The optimal policy can be found once we have found the optimal state-value function 
$v_*$ and the optimal action-value function $q_*$.
Since $v_*$ is the value function for a policy, it must satisfy the Bellman equation 
for {\bf state-values}. Moreover, it is also the optimal value function, then we have 
\begin{align} 
v_*(s)&=\max_{a\in\mathcal{A}(s)}q_{\pi_*}(s,a)  \notag \\
&=\max_{a}\mathbb{E}_{\pi_*}[G_t|S_t=s,A_t=a]  \notag \\
&=\max_{a}\mathbb{E}_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a]  \notag \\
&=\max_{a}\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a]  \label{mdp_bellman_optimality_equation_1} \\
&=\max_{a}\sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]  \label{mdp_bellman_optimality_equation_2}  
\end{align} 
The last two equations are two forms of the Bellman optimality equation for $v_*$. 
Similarly, we have the Bellman optimality equation for {\bf action-value} $q_*$ 
\begin{align} 
q_*(s,a)&=\mathbb{E}\left[R_{t+1}+\gamma\max_{a'}q_*(S_{t+1},a')|S_t=s,A_t=a\right] \\
&=\sum_{s',r}p(s',r|s,a)\left[r+\gamma\max_{a'}q_*(s',a')\right] 
\end{align}


\subsection{Bellman Optimality Equation vs Optimal Policy}
% https://yangyangfu.github.io/learning/reinforcement%20learning/2020/05/09/Bellman-Equation/

The ultimate goal of RL is to find the optimal policy. But how does the Bellman 
optimality equation relate to the optimal policy?

Once we find out the optimal state-value $v_*$, it is relatively easy to determine 
an optimal policy. For each state $s$, there will be one or more actions at which 
the maximum is obtained from the Bellman optimality equation. Any policy that assigns 
nonzero probability only to these actions is an optimal policy. You can think of this 
as a one-step search. If we have the optimal state-value function $v_*$, then the 
actions that appear best after a one-step search will be optimal actions.

Let's now retreat all these in another way: any policy that is greedy with respect to 
the optimal evaluation function $v_*$ is an optimal policy. The term greedy is used 
in computer science to describe any search or decision procedure that selects 
alternatives based only on local or immediate considerations, without considering the 
possibility that such a selection may prevent future access to even better alternatives. 
Consequently, it describes policies that select actions based only on their short-term 
consequences. The beauty of $v_*$ is that if one uses it to evaluate the short-term 
consequences of actions—specifically, the one-step consequences—then a greedy policy is 
actually optimal in the long-term sense in which we are interested because $v_*$ already 
takes into consideration the reward consequences of all possible future behavior. By 
means of $v_*$, the optimal expected long-term return is turned into a quantity that is 
locally and immediately available for each state. Hence, a one-step-ahead search yields 
the long-term optimal actions.

Having $q_*$ makes choosing optimal actions even easier. With $q_*$, the agent does not 
even have to do a one-step-ahead search: for any state $s$, it can simply find any 
action that maximizes $q_*(s, a)$. The action-value function actively caches the results 
of all one-step-ahead searches. It provides the optimal expected long-term return as a 
value that is locally and immediately available for each state–action pair. Hence at the 
cost of representing a function of state–action pairs, instead of just of states, the 
optimal actionvalue function allows optimal actions to be selected without having to 
know anything about possible successor states and their values, that is, without having 
to know anything about the environment's dynamics.



\subsection{Backup diagram for $v_*$ and $q_*$}


\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/opt.png}
\caption{backup diagram for $v_*$ and $q_*$}
%\label{fig:label}
\end{figure}


\subsection{References}

\cite{silver2015}

% https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html

% https://deepmind.com/research/case-studies/alphago-the-story-so-far


\section{Canonical Example: Grid World}

\begin{itemize}
%\setlength{\itemsep}{0pt}
%\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item
The agent lives in a grid, figure \ref{fig:mdp_Grid_world}

\item
Walls block the agent's path

\item
The agent's actions do not always go as planned:
    \begin{itemize}
    \item
    $80\%$ of the time, the action North takes the agent North(if there is no wall there)

    \item
    $10\%$ of the time, North takes the agent West; $10\%$ East

    \item
    If there is a wall in the direction the agent would have been taken, the agent stays put

    \end{itemize}

\item
Big rewards come at the end

\end{itemize}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/mdp/Grid_world.png}
\caption{Canonical Example: Grid world}
\label{fig:mdp_Grid_world}
\end{figure}

{\bf Solving MDPs}

\begin{itemize}
\item
In an MDP, we want an optimal \textcolor{red}{policy $\pi^*: S\times 0: H\rightarrow A$}

    \begin{itemize}
    \item
    A policy $\pi$ gives an action for each state for each time, figure \ref{fig:action_for_each_state}

    \begin{figure}[!htb]
    \centering
    \includegraphics[scale=0.618]{pix/mdp/action_for_each_state.png}
    \caption{Canonical Example: action for each state}
    \label{fig:action_for_each_state}
    \end{figure}

    \item
    An optimal policy maximizes expected sum of rewards

    \end{itemize}

\item
Contrast: In deterministic, want an optimal \textcolor{magenta}{plan}, or sequence 
of actions, from start to a goal
\end{itemize}

Optimal Control

$=$

given an MDP $(\mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, \gamma, H)$

find the optimal policy $\pi^*$

Exact Methods:
\begin{itemize}
\item
Value Iteration

\item
Policy Iteration

\item
Linear Programming
\end{itemize}


\subsection{Value iteration}

Algorithm:
\begin{itemize}
\item
Start with $V_0^*(s)=0$ for all $s$.

\item
For $i=1,\ldots, H$ \\
Given $V_i^*$, calculate for all states $s\in\mathcal{S}$: \\
\textcolor{magenta}{
$$
V_{i+1}^*(s) \leftarrow \max_a\sum_{s'}T(s,a,s')\left[
R(s,a,s') + V_i^*(s') \right]
$$
}

\item
This is called a \textcolor{magenta}{value update} or \textcolor{magenta}{
Bellman update/back-up}
\end{itemize}

\noindent where $V_i^*(s)$ is the expected sum of rewards accumulated when 
starting from state $s$ and acting optimally for 
\textcolor{red}{\bf a horizon of $i$ steps}


\subsection{Value iteration in Gridworld}

$\text{noise} = 0.2$, $\gamma=0.9$, two terminal states with $R=+1$ and $-1$. 

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/mdp/grid_world_value_iteration_1.png}
\caption{Grid World: Values after 1 iteration}
%\label{fig:action_for_each_state}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/mdp/grid_world_value_iteration_2.png}
\caption{Grid World: Values after 2 iterations}
%\label{fig:action_for_each_state}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/mdp/grid_world_value_iteration_3.png}
\caption{Grid World: Values after 3 iterations}
%\label{fig:action_for_each_state}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/mdp/grid_world_value_iteration_4.png}
\caption{Grid World: Values after 4 iterations}
%\label{fig:action_for_each_state}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/mdp/grid_world_value_iteration_5.png}
\caption{Grid World: Values after 5 iterations}
%\label{fig:action_for_each_state}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/mdp/grid_world_value_iteration_100.png}
\caption{Grid World: Values after 100 iterations}
%\label{fig:action_for_each_state}
\end{figure}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.618]{pix/mdp/grid_world_value_iteration_100.png}
\caption{Grid World: Values after 1000 iterations}
%\label{fig:action_for_each_state}
\end{figure}

{\bf \textcolor{magenta}{当前进展总结：}}
\begin{itemize}
%\setlength{\itemsep}{0pt}
%\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item[-]
当前能正确运行的代码的位置 $D:/mygit/reinforcement\_mdp/gridworld.py$

\item[-]
参数的设置位于 gridworld.py 的 ln494 及之后的一段

\item[-]
代码的来源及说明参见(修改后的代码运行结果经过检测满足Question 1的要求) \\
\url{https://courses.cs.washington.edu/courses/csep573/19wi/assignments/reinforcement_mdp.html}
\\
网页被另存于本地位置 \\
\url{file:///D:/webPages/mdp/Assignment%203_%20Markov%20Decision%20Processes%20-%20CSE%20P%20573.html}

\item[-]
代码修改的依据参见 \\
\url{https://gitee.com/industry-ai/mdp}

\end{itemize}


\subsection{Example Analysis}

% https://hub.packtpub.com/reinforcement-learning-mdp-markov-decision-process-tutorial/

The Markov decision process, better known as MDP, is an approach in reinforcement 
learning to take decisions in a gridworld environment. A gridworld environment 
consists of states in the form of grids.

The MDP tries to capture a world in the form of a grid by dividing it into states, 
actions, models/transition models, and rewards. The solution to an MDP is called 
a policy and the objective is to find the optimal policy for that MDP task.

Thus, any reinforcement learning task composed of a set of states, actions, and 
rewards that follows the Markov property would be considered an MDP.

In this subsection, we will dig deep into MDPs, states, actions, rewards, policies, 
and how to solve them using Bellman equations through the above gridworld example.

\subsubsection{Markov decision processes}

这里复习一下MDP的定义 \ref{def_FMDP}

MDP is defined as the collection of the following:
\begin{itemize}
%\setlength{\itemsep}{0pt}
%\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item[-]
{\bf States:} $\mathcal{S}$

\item[-]
{\bf Actions:} $\mathcal{A}$

\item[-]
{\bf Transition model:} $\mathcal{T}: \mathcal{S}\times\mathcal{A}\times\mathcal{S}\times\{0,1,\ldots,H\}
\rightarrow [0,1]$ \\
$\mathcal{T}_t(s,a,s')=P(s_{t+1}=s'|s_t=s,a_t=a)$ \\
$T(s,a,s')\sim P(s'|s,a)$ 

\item[-]
{\bf Rewards:} $R$: \mathcal{S}\times\mathcal{A}\times\mathcal{S}\times\{0,1,\ldots,H\}
\rightarrow \mathcal{R} \\
$R_t(s,a,s')=\text{reward for} (s_{t+1}=s', s_t=s, a_t=a)$ \\
$R(s), R(s,a), R(s,a,s')$

\item[-]
{\bf Policy:} 
$\pi: \mathcal{S}\times\{0,1,\ldots,H\}\rightarrow \mathcal{A}$ \\
$\pi(s) \rightarrow \alpha$ \\
$\pi^*$ is the optimal policy

\end{itemize}

In the case of an MDP, the environment is fully observable, that is, whatever 
observation the agent makes at any point in time is enough to make an optimal 
decision. In case of a partially observable environment, the agent needs a 
memory to store the past observations to make the best possible decisions.

Let's try to break this into different lego blocks to understand what this 
overall process means.

\subsubsection{The Markov property}

In short, as per the {\bf Markov property}, in order to know the information 
of near future (say, at time $t+1$) the present information at time $t$ matters.

Given a sequence, $[x_1, x_2, \ldots, x_t]$, the first order of Markov says,
$P(x_t | x_{t-1}, x_{t-2}, \ldots, x_1) = P(x_t | x_{t-1})$, that is,
$x_t$ depends only on $x_{t-1}$. Therefore, $x_{t+1}$ will depend only on $x_t$. 
The second order of Markov says, $P(x_t | x_{t-1}, x_{t-2}, \ldots, x_1) = 
P(x_t | x_{t-1}, x_{t-2})$, that is, $x_t$ depends only on $x_{t-1}$ and 
$x_{t-2}$.

In our context, we will follow the first order of the Markov property from now 
on. Therefore, we can convert any process to a Markov property if the probability 
of the new state, say $x_{t+1}$, depends only on the current state, $x_t$, such 
that the current state captures and remembers the property and knowledge from the 
past. Thus, as per the Markov property, the world (that is, the environment) is 
considered to be stationary, that is, the rules in the world are fixed.






